## NLP 的技术途径与核心步骤

NLP 可以使用传统的机器学习方法来处理，也可以使用深度学习的方法来处理。2 种不同的途径也对应着不同的处理步骤。让我们深入了解这两种方法的细节：

### 传统机器学习 vs. 深度学习途径

#### 传统机器学习流程

![image-20250614145321255](./img/10.png)

传统机器学习方法的处理流程包括：

1. **语料预处理**
   - **数据清洗**：去除HTML标签、特殊字符、无意义字符等
   - **分词处理**：将文本切分为最小的语义单位
   - **特征提取**：
     - 词频统计（TF）：计算词在文档中出现的频率
     - TF-IDF：考虑词在整个语料库中的重要性
     - N-gram特征：捕捉词序信息
   
2. **特征工程**
   - **特征选择**：
     - 词性标注：名词、动词、形容词等
     - 句法分析：主谓宾关系、从属关系等
     - 实体识别：人名、地名、组织机构名等
   - **特征组合**：
     - 语义特征：同义词、反义词关系
     - 上下文特征：窗口大小的选择
     - 统计特征：词频、文档频率等

3. **模型选择**
   - 朴素贝叶斯：适用于文本分类
   - 支持向量机（SVM）：处理高维特征
   - 决策树/随机森林：可解释性强
   - 最大熵模型：处理多分类问题

**优势**：
- 可解释性强，每个特征的作用清晰可见
- 适合小规模标注数据
- 训练速度快，计算资源需求小
- 特征可以融入专家领域知识

**局限性**：
- 特征设计依赖专家经验
- 特征工程耗时耗力
- 泛化能力有限
- 难以处理长距离依赖

#### 深度学习流程

![image-20250614145321255](./img/11.png)

深度学习方法的处理流程：

1. **语料预处理**
   - **文本清洗**：同传统方法
   - **分词/子词切分**：
     - WordPiece
     - BPE（字节对编码）
     - SentencePiece
   - **分布式表示**：
     - Word2Vec：CBOW和Skip-gram
     - GloVe：全局词向量
     - BERT词向量：上下文相关的动态表示

2. **模型设计**
   - **基础架构**：
     - CNN：捕捉局部特征
     - RNN/LSTM/GRU：处理序列信息
     - Transformer：自注意力机制
   - **预训练模型**：
     - BERT：双向编码器
     - GPT：自回归生成
     - T5：文本到文本转换
   - **任务适配**：
     - 微调策略
     - 提示学习
     - 少样本学习

3. **训练优化**
   - 学习率调度
   - 梯度裁剪
   - 正则化技术
   - 早停策略

**优势**：
- 自动特征学习
- 端到端训练
- 强大的表达能力
- 适用于大规模数据

**局限性**：
- 需要大量训练数据
- 计算资源要求高
- 可解释性较弱
- 训练不稳定

### 语料预处理核心步骤

#### 英文NLP预处理（6步）

![image-20250614145321255](./img/12.png)

1. **分词（Tokenization）**
   - **规则**：基于空格、标点符号
   - **特殊处理**：
     - 缩写：don't → do not
     - 数字：格式统一化
     - 邮箱/URL：特殊标记
   - **示例**："I love NLP!" → ["I", "love", "NLP", "!"]

2. **词干提取（Stemming）**
   - **算法选择**：
     - Porter词干提取
     - Lancaster词干提取
     - Snowball词干提取
   - **示例**：
     - "running" → "run"
     - "flies" → "fli"
     - "better" → "bet"

3. **词形还原（Lemmatization）**
   - **基于词典**：WordNet
   - **考虑词性**：动词、名词变化
   - **示例**：
     - "am/is/are" → "be"
     - "better" → "good"
     - "went" → "go"

4. **词性标注（POS Tagging）**
   - **标准**：Penn Treebank标记集
   - **方法**：
     - 规则based
     - 统计based
     - 神经网络based
   - **示例**：["The", "cat", "sits"] → [DT, NN, VBZ]

5. **命名实体识别（NER）**
   - **实体类型**：
     - 人名（PER）
     - 地名（LOC）
     - 组织机构（ORG）
     - 时间（TIME）
     - 数量（QUANTITY）
   - **示例**："Google was founded in Mountain View" → [Google/ORG, Mountain View/LOC]

6. **分块（Chunking）**
   - **短语类型**：
     - 名词短语（NP）
     - 动词短语（VP）
     - 介词短语（PP）
   - **示例**："[NP the quick brown fox] [VP jumps over] [NP the lazy dog]"

#### 中文NLP预处理（4步）

![image-20250614145321255](./img/13.png)

1. **中文分词**
   - **算法类型**：
     - 基于规则：正向最大匹配、逆向最大匹配
     - 基于统计：HMM、CRF
     - 基于深度学习：BiLSTM+CRF
   - **工具选择**：
     - jieba分词
     - THULAC
     - LTP
   - **示例**："我爱自然语言处理" → ["我", "爱", "自然语言处理"]

2. **词性标注**
   - **标记集**：
     - 北大标记集
     - 宾州中文树库标记集
   - **特殊处理**：
     - 多音字消歧
     - 未登录词处理
   - **示例**：["我/r", "爱/v", "自然语言处理/n"]

3. **命名实体识别**
   - **中文特点**：
     - 无空格分隔
     - 嵌套实体
     - 新词实体
   - **识别难点**：
     - 人名识别（单字名、复姓）
     - 地名识别（简称、全称）
     - 机构名识别（简称、别称）
   - **示例**："李明在北京大学学习" → [李明/人名, 北京大学/机构名]

4. **停用词处理**
   - **停用词表**：
     - 常用词表
     - 领域特定词表
   - **处理策略**：
     - 完全过滤
     - 条件过滤
     - 权重调整
   - **示例**：过滤"的"、"了"、"在"等虚词

### 预处理技巧与注意事项

1. **数据质量保证**
   - 文本编码统一（UTF-8）
   - 异常字符处理
   - 数据去重

2. **领域适配**
   - 专业词典构建
   - 自定义分词规则
   - 特定实体识别

3. **效率优化**
   - 并行处理
   - 缓存机制
   - 增量更新

4. **评估与调优**
   - 分词准确率
   - 实体识别F1值
   - 词性标注正确率

### 总结

NLP的技术路径选择和预处理步骤直接影响后续任务的效果。传统机器学习和深度学习各有优势，在实际应用中常常需要结合使用。预处理步骤虽然基础，但需要细致处理，尤其要注意语言特点（如中英文差异）和领域特征。随着预训练模型的发展，一些预处理步骤（如分词）的实现方式也在发生变化，但其重要性始终不变。